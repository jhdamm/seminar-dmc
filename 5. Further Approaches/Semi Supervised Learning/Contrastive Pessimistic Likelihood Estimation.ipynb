{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "train=pd.read_csv('../../1. Task and Data/train.csv',sep='|')\n",
    "test=pd.read_csv('../../1. Task and Data/test.csv',sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training set\n",
    "\n",
    "train['scannedLineItems'] = train['scannedLineItemsPerSecond'] * train['totalScanTimeInSeconds']\n",
    "train['pricePerScannedLineItem'] = train['grandTotal'] / train['scannedLineItems']\n",
    "train['scansWithoutRegistrationPerScannedLineItem'] = train['scansWithoutRegistration'] / train['scannedLineItems']\n",
    "train['quantityModificationsPerScannedLineItem'] = train['quantityModifications'] / train['scannedLineItems']\n",
    "train['lineItemVoidsPerSecond'] = train['lineItemVoids'] / train['totalScanTimeInSeconds']\n",
    "train['scansWithoutRegistrationPerSecond'] = train['scansWithoutRegistration'] / train['totalScanTimeInSeconds']\n",
    "train['quantityModificationsPerSecond'] = train['quantityModifications'] / train['totalScanTimeInSeconds']\n",
    "train['secondsPerEuro'] = train['totalScanTimeInSeconds'] / train['grandTotal']\n",
    "train['lineItemVoidsPerEuro'] = train['lineItemVoids'] / train['grandTotal']\n",
    "train['scansWithoutRegistrationPerEuro'] = train['scansWithoutRegistration'] / train['grandTotal']\n",
    "train['quantityModificationsPerEuro'] = train['quantityModifications'] / train['grandTotal']\n",
    "\n",
    "\n",
    "# for test set\n",
    "test['scannedLineItems'] = test['scannedLineItemsPerSecond'] * test['totalScanTimeInSeconds']\n",
    "test['pricePerScannedLineItem'] = test['grandTotal'] / test['scannedLineItems']\n",
    "test['scansWithoutRegistrationPerScannedLineItem'] = test['scansWithoutRegistration'] / test['scannedLineItems']\n",
    "test['quantityModificationsPerScannedLineItem'] = test['quantityModifications'] / test['scannedLineItems']\n",
    "test['lineItemVoidsPerSecond'] = test['lineItemVoids'] / test['totalScanTimeInSeconds']\n",
    "test['scansWithoutRegistrationPerSecond'] = test['scansWithoutRegistration'] / test['totalScanTimeInSeconds']\n",
    "test['quantityModificationsPerSecond'] = test['quantityModifications'] / test['totalScanTimeInSeconds']\n",
    "test['secondsPerEuro'] = test['totalScanTimeInSeconds'] / test['grandTotal']\n",
    "test['lineItemVoidsPerEuro'] = test['lineItemVoids'] / test['grandTotal']\n",
    "test['scansWithoutRegistrationPerEuro'] = test['scansWithoutRegistration'] / test['grandTotal']\n",
    "test['quantityModificationsPerEuro'] = test['quantityModifications'] / test['grandTotal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trustLevel</th>\n",
       "      <th>totalScanTimeInSeconds</th>\n",
       "      <th>grandTotal</th>\n",
       "      <th>lineItemVoids</th>\n",
       "      <th>scansWithoutRegistration</th>\n",
       "      <th>quantityModifications</th>\n",
       "      <th>scannedLineItemsPerSecond</th>\n",
       "      <th>valuePerSecond</th>\n",
       "      <th>lineItemVoidsPerPosition</th>\n",
       "      <th>scannedLineItems</th>\n",
       "      <th>pricePerScannedLineItem</th>\n",
       "      <th>scansWithoutRegistrationPerScannedLineItem</th>\n",
       "      <th>quantityModificationsPerScannedLineItem</th>\n",
       "      <th>lineItemVoidsPerSecond</th>\n",
       "      <th>scansWithoutRegistrationPerSecond</th>\n",
       "      <th>quantityModificationsPerSecond</th>\n",
       "      <th>secondsPerEuro</th>\n",
       "      <th>lineItemVoidsPerEuro</th>\n",
       "      <th>scansWithoutRegistrationPerEuro</th>\n",
       "      <th>quantityModificationsPerEuro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>467</td>\n",
       "      <td>88.48</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.014989</td>\n",
       "      <td>0.189465</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.640000</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.008565</td>\n",
       "      <td>0.017131</td>\n",
       "      <td>0.008565</td>\n",
       "      <td>5.278029</td>\n",
       "      <td>0.045208</td>\n",
       "      <td>0.090416</td>\n",
       "      <td>0.045208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1004</td>\n",
       "      <td>58.99</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.026892</td>\n",
       "      <td>0.058755</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.184815</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.006972</td>\n",
       "      <td>0.005976</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>17.019834</td>\n",
       "      <td>0.118664</td>\n",
       "      <td>0.101712</td>\n",
       "      <td>0.016952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>162</td>\n",
       "      <td>14.00</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006173</td>\n",
       "      <td>0.086420</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>0.030864</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>11.571429</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>532</td>\n",
       "      <td>84.79</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.159380</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.056429</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.016917</td>\n",
       "      <td>0.005639</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>6.274325</td>\n",
       "      <td>0.106145</td>\n",
       "      <td>0.035382</td>\n",
       "      <td>0.047175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>890</td>\n",
       "      <td>42.16</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021348</td>\n",
       "      <td>0.047371</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.218947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.110057</td>\n",
       "      <td>0.094877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trustLevel  totalScanTimeInSeconds  grandTotal  lineItemVoids  scansWithoutRegistration  quantityModifications  scannedLineItemsPerSecond  valuePerSecond  lineItemVoidsPerPosition  scannedLineItems  pricePerScannedLineItem  scansWithoutRegistrationPerScannedLineItem  quantityModificationsPerScannedLineItem  lineItemVoidsPerSecond  scansWithoutRegistrationPerSecond  quantityModificationsPerSecond  secondsPerEuro  lineItemVoidsPerEuro  scansWithoutRegistrationPerEuro  quantityModificationsPerEuro\n",
       "0           4                     467       88.48              4                         8                      4                   0.014989        0.189465                  0.571429               7.0                12.640000                                    1.142857                                 0.571429                0.008565                           0.017131                        0.008565        5.278029              0.045208                         0.090416                      0.045208\n",
       "1           3                    1004       58.99              7                         6                      1                   0.026892        0.058755                  0.259259              27.0                 2.184815                                    0.222222                                 0.037037                0.006972                           0.005976                        0.000996       17.019834              0.118664                         0.101712                      0.016952\n",
       "2           1                     162       14.00              4                         5                      4                   0.006173        0.086420                  4.000000               1.0                14.000000                                    5.000000                                 4.000000                0.024691                           0.030864                        0.024691       11.571429              0.285714                         0.357143                      0.285714\n",
       "3           5                     532       84.79              9                         3                      4                   0.026316        0.159380                  0.642857              14.0                 6.056429                                    0.214286                                 0.285714                0.016917                           0.005639                        0.007519        6.274325              0.106145                         0.035382                      0.047175\n",
       "4           5                     890       42.16              4                         0                      0                   0.021348        0.047371                  0.210526              19.0                 2.218947                                    0.000000                                 0.000000                0.004494                           0.000000                        0.000000       21.110057              0.094877                         0.000000                      0.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation for perEuro attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40562            inf\n",
       "492399           inf\n",
       "86833            inf\n",
       "363211           inf\n",
       "456994           inf\n",
       "155622           inf\n",
       "438266           inf\n",
       "69401            inf\n",
       "489766           inf\n",
       "147774           inf\n",
       "52962            inf\n",
       "34895            inf\n",
       "143886           inf\n",
       "129034           inf\n",
       "87373            inf\n",
       "199015           inf\n",
       "117536           inf\n",
       "462632           inf\n",
       "436235           inf\n",
       "328053           inf\n",
       "112020           inf\n",
       "29990            inf\n",
       "120191           inf\n",
       "214052           inf\n",
       "208504           inf\n",
       "30187            inf\n",
       "454691           inf\n",
       "19792            inf\n",
       "46561            inf\n",
       "270427           inf\n",
       "30423            inf\n",
       "158772           inf\n",
       "496219           inf\n",
       "146252           inf\n",
       "455867           inf\n",
       "337680           inf\n",
       "161945           inf\n",
       "64223            inf\n",
       "201272           inf\n",
       "461791           inf\n",
       "13695            inf\n",
       "211326           inf\n",
       "118032           inf\n",
       "91248            inf\n",
       "179013           inf\n",
       "455538           inf\n",
       "441018           inf\n",
       "330870           inf\n",
       "106855           inf\n",
       "389520           inf\n",
       "43649            inf\n",
       "287909           inf\n",
       "35561            inf\n",
       "418922           inf\n",
       "190225           inf\n",
       "299044           inf\n",
       "87759            inf\n",
       "53667     500.000000\n",
       "133426    500.000000\n",
       "497223    500.000000\n",
       "31676     500.000000\n",
       "102618    400.000000\n",
       "371320    400.000000\n",
       "283861    400.000000\n",
       "3282      400.000000\n",
       "176008    400.000000\n",
       "16726     300.000000\n",
       "448922    300.000000\n",
       "156517    300.000000\n",
       "236301    300.000000\n",
       "158961    300.000000\n",
       "428300    300.000000\n",
       "129930    300.000000\n",
       "256995    300.000000\n",
       "58189     300.000000\n",
       "104670    300.000000\n",
       "306285    250.000000\n",
       "449290    250.000000\n",
       "166233    250.000000\n",
       "175423    250.000000\n",
       "65523     250.000000\n",
       "57386     250.000000\n",
       "356924    250.000000\n",
       "416085    250.000000\n",
       "234038    200.000000\n",
       "149198    200.000000\n",
       "124665    200.000000\n",
       "16134     200.000000\n",
       "227115    200.000000\n",
       "306866    200.000000\n",
       "487253    200.000000\n",
       "431434    200.000000\n",
       "24165     200.000000\n",
       "250987    200.000000\n",
       "16998     200.000000\n",
       "178268    200.000000\n",
       "232443    200.000000\n",
       "267039    200.000000\n",
       "39654     200.000000\n",
       "207351    200.000000\n",
       "481847    200.000000\n",
       "157456    200.000000\n",
       "25121     200.000000\n",
       "304527    200.000000\n",
       "7624      200.000000\n",
       "429833    200.000000\n",
       "309508    200.000000\n",
       "374213    200.000000\n",
       "373593    200.000000\n",
       "23336     200.000000\n",
       "154649    166.666667\n",
       "447408    166.666667\n",
       "302576    166.666667\n",
       "101060    166.666667\n",
       "150499    166.666667\n",
       "223221    166.666667\n",
       "128211    166.666667\n",
       "286758    166.666667\n",
       "103050    150.000000\n",
       "469526    150.000000\n",
       "134886    150.000000\n",
       "68346     150.000000\n",
       "22192     150.000000\n",
       "50403     150.000000\n",
       "14751     150.000000\n",
       "251164    150.000000\n",
       "9273      150.000000\n",
       "195198    150.000000\n",
       "228560    133.333333\n",
       "315680    133.333333\n",
       "186307    133.333333\n",
       "354611    133.333333\n",
       "380463    133.333333\n",
       "252849    133.333333\n",
       "486833    133.333333\n",
       "35380     133.333333\n",
       "486320    133.333333\n",
       "211776    133.333333\n",
       "184479    133.333333\n",
       "82156     133.333333\n",
       "136661    133.333333\n",
       "3653      133.333333\n",
       "456458    125.000000\n",
       "258338    125.000000\n",
       "48123     125.000000\n",
       "81978     125.000000\n",
       "280337    125.000000\n",
       "215024    125.000000\n",
       "288380    125.000000\n",
       "400587    125.000000\n",
       "195437    125.000000\n",
       "216483    125.000000\n",
       "484382    125.000000\n",
       "215335    100.000000\n",
       "187633    100.000000\n",
       "103595    100.000000\n",
       "138846    100.000000\n",
       "269421    100.000000\n",
       "71060     100.000000\n",
       "470823    100.000000\n",
       "310894    100.000000\n",
       "353291    100.000000\n",
       "35932     100.000000\n",
       "160837    100.000000\n",
       "242300    100.000000\n",
       "347490    100.000000\n",
       "389981    100.000000\n",
       "458442    100.000000\n",
       "285196    100.000000\n",
       "396734    100.000000\n",
       "261011    100.000000\n",
       "347724    100.000000\n",
       "286247    100.000000\n",
       "70209     100.000000\n",
       "454704    100.000000\n",
       "398228    100.000000\n",
       "257586    100.000000\n",
       "422628    100.000000\n",
       "256054    100.000000\n",
       "126292    100.000000\n",
       "294177    100.000000\n",
       "278522    100.000000\n",
       "7605      100.000000\n",
       "405388    100.000000\n",
       "457265    100.000000\n",
       "427150    100.000000\n",
       "372795    100.000000\n",
       "13725     100.000000\n",
       "372754    100.000000\n",
       "332910    100.000000\n",
       "391369    100.000000\n",
       "206802     83.333333\n",
       "445151     83.333333\n",
       "306261     83.333333\n",
       "451236     83.333333\n",
       "446393     83.333333\n",
       "349243     83.333333\n",
       "219308     83.333333\n",
       "116391     83.333333\n",
       "434975     83.333333\n",
       "485587     83.333333\n",
       "228967     80.000000\n",
       "351935     80.000000\n",
       "97101      80.000000\n",
       "233295     80.000000\n",
       "314855     75.000000\n",
       "455728     75.000000\n",
       "33592      75.000000\n",
       "241432     75.000000\n",
       "116881     75.000000\n",
       "467226     75.000000\n",
       "292051     75.000000\n",
       "201233     75.000000\n",
       "416801     75.000000\n",
       "444529     71.428571\n",
       "379820     71.428571\n",
       "186711     71.428571\n",
       "34321      71.428571\n",
       "378908     71.428571\n",
       "214139     71.428571\n",
       "452081     71.428571\n",
       "42831      71.428571\n",
       "150453     71.428571\n",
       "345096     71.428571\n",
       "432282     66.666667\n",
       "269630     66.666667\n",
       "166380     66.666667\n",
       "294199     66.666667\n",
       "205210     66.666667\n",
       "85390      66.666667\n",
       "428532     66.666667\n",
       "247837     66.666667\n",
       "382411     66.666667\n",
       "60423      66.666667\n",
       "466514     66.666667\n",
       "71334      66.666667\n",
       "151093     66.666667\n",
       "50267      66.666667\n",
       "61186      66.666667\n",
       "25381      66.666667\n",
       "481154     66.666667\n",
       "112396     66.666667\n",
       "105383     66.666667\n",
       "275316     66.666667\n",
       "12561      66.666667\n",
       "184687     66.666667\n",
       "480748     66.666667\n",
       "120256     62.500000\n",
       "320373     62.500000\n",
       "335024     62.500000\n",
       "             ...    \n",
       "116522      0.000000\n",
       "116530      0.000000\n",
       "116532      0.000000\n",
       "116534      0.000000\n",
       "397156      0.000000\n",
       "116542      0.000000\n",
       "116543      0.000000\n",
       "116545      0.000000\n",
       "116555      0.000000\n",
       "240179      0.000000\n",
       "396802      0.000000\n",
       "116957      0.000000\n",
       "396800      0.000000\n",
       "240081      0.000000\n",
       "117355      0.000000\n",
       "117357      0.000000\n",
       "396447      0.000000\n",
       "396446      0.000000\n",
       "117359      0.000000\n",
       "239976      0.000000\n",
       "396441      0.000000\n",
       "396440      0.000000\n",
       "396437      0.000000\n",
       "117370      0.000000\n",
       "117376      0.000000\n",
       "117382      0.000000\n",
       "117383      0.000000\n",
       "117385      0.000000\n",
       "117386      0.000000\n",
       "396417      0.000000\n",
       "239972      0.000000\n",
       "239966      0.000000\n",
       "239959      0.000000\n",
       "117408      0.000000\n",
       "289468      0.000000\n",
       "117421      0.000000\n",
       "396394      0.000000\n",
       "396387      0.000000\n",
       "396385      0.000000\n",
       "289452      0.000000\n",
       "396453      0.000000\n",
       "117339      0.000000\n",
       "117288      0.000000\n",
       "396525      0.000000\n",
       "117257      0.000000\n",
       "396523      0.000000\n",
       "396521      0.000000\n",
       "117266      0.000000\n",
       "117269      0.000000\n",
       "117273      0.000000\n",
       "117284      0.000000\n",
       "396509      0.000000\n",
       "117285      0.000000\n",
       "117286      0.000000\n",
       "117298      0.000000\n",
       "117337      0.000000\n",
       "396496      0.000000\n",
       "117310      0.000000\n",
       "239988      0.000000\n",
       "117318      0.000000\n",
       "117323      0.000000\n",
       "396478      0.000000\n",
       "396477      0.000000\n",
       "396472      0.000000\n",
       "117330      0.000000\n",
       "396464      0.000000\n",
       "396461      0.000000\n",
       "396384      0.000000\n",
       "396382      0.000000\n",
       "117438      0.000000\n",
       "396285      0.000000\n",
       "117531      0.000000\n",
       "117532      0.000000\n",
       "117537      0.000000\n",
       "396300      0.000000\n",
       "396299      0.000000\n",
       "117547      0.000000\n",
       "117549      0.000000\n",
       "396295      0.000000\n",
       "239936      0.000000\n",
       "396292      0.000000\n",
       "117552      0.000000\n",
       "396282      0.000000\n",
       "117528      0.000000\n",
       "396279      0.000000\n",
       "289496      0.000000\n",
       "117576      0.000000\n",
       "396273      0.000000\n",
       "117579      0.000000\n",
       "289499      0.000000\n",
       "239927      0.000000\n",
       "396252      0.000000\n",
       "396251      0.000000\n",
       "117599      0.000000\n",
       "117600      0.000000\n",
       "239939      0.000000\n",
       "289486      0.000000\n",
       "117439      0.000000\n",
       "117491      0.000000\n",
       "239957      0.000000\n",
       "396372      0.000000\n",
       "117455      0.000000\n",
       "396368      0.000000\n",
       "117459      0.000000\n",
       "117462      0.000000\n",
       "117467      0.000000\n",
       "239949      0.000000\n",
       "396357      0.000000\n",
       "117476      0.000000\n",
       "117488      0.000000\n",
       "289479      0.000000\n",
       "396322      0.000000\n",
       "117493      0.000000\n",
       "396341      0.000000\n",
       "117499      0.000000\n",
       "396339      0.000000\n",
       "117502      0.000000\n",
       "239948      0.000000\n",
       "396335      0.000000\n",
       "117506      0.000000\n",
       "117509      0.000000\n",
       "117511      0.000000\n",
       "117519      0.000000\n",
       "396526      0.000000\n",
       "396528      0.000000\n",
       "117255      0.000000\n",
       "117074      0.000000\n",
       "117039      0.000000\n",
       "117041      0.000000\n",
       "117042      0.000000\n",
       "240059      0.000000\n",
       "289385      0.000000\n",
       "396716      0.000000\n",
       "117057      0.000000\n",
       "117064      0.000000\n",
       "117068      0.000000\n",
       "240052      0.000000\n",
       "396702      0.000000\n",
       "396699      0.000000\n",
       "240067      0.000000\n",
       "117081      0.000000\n",
       "117082      0.000000\n",
       "117088      0.000000\n",
       "396685      0.000000\n",
       "396682      0.000000\n",
       "117099      0.000000\n",
       "117104      0.000000\n",
       "117107      0.000000\n",
       "117108      0.000000\n",
       "289397      0.000000\n",
       "396672      0.000000\n",
       "240066      0.000000\n",
       "396736      0.000000\n",
       "117121      0.000000\n",
       "116996      0.000000\n",
       "396794      0.000000\n",
       "116966      0.000000\n",
       "396791      0.000000\n",
       "116968      0.000000\n",
       "116973      0.000000\n",
       "396776      0.000000\n",
       "396775      0.000000\n",
       "396774      0.000000\n",
       "396773      0.000000\n",
       "116986      0.000000\n",
       "240079      0.000000\n",
       "240077      0.000000\n",
       "396737      0.000000\n",
       "117004      0.000000\n",
       "396757      0.000000\n",
       "289370      0.000000\n",
       "289371      0.000000\n",
       "117013      0.000000\n",
       "396751      0.000000\n",
       "289373      0.000000\n",
       "396749      0.000000\n",
       "117016      0.000000\n",
       "396741      0.000000\n",
       "117027      0.000000\n",
       "117111      0.000000\n",
       "117126      0.000000\n",
       "117246      0.000000\n",
       "240017      0.000000\n",
       "396589      0.000000\n",
       "396585      0.000000\n",
       "117194      0.000000\n",
       "117195      0.000000\n",
       "396581      0.000000\n",
       "117208      0.000000\n",
       "240026      0.000000\n",
       "240022      0.000000\n",
       "396574      0.000000\n",
       "117212      0.000000\n",
       "396572      0.000000\n",
       "396568      0.000000\n",
       "396596      0.000000\n",
       "289424      0.000000\n",
       "396564      0.000000\n",
       "396562      0.000000\n",
       "117220      0.000000\n",
       "396555      0.000000\n",
       "396553      0.000000\n",
       "289427      0.000000\n",
       "240015      0.000000\n",
       "240014      0.000000\n",
       "396544      0.000000\n",
       "240009      0.000000\n",
       "117192      0.000000\n",
       "289418      0.000000\n",
       "396657      0.000000\n",
       "117142      0.000000\n",
       "396656      0.000000\n",
       "396655      0.000000\n",
       "396653      0.000000\n",
       "289401      0.000000\n",
       "117132      0.000000\n",
       "396646      0.000000\n",
       "117133      0.000000\n",
       "289402      0.000000\n",
       "396643      0.000000\n",
       "117134      0.000000\n",
       "396640      0.000000\n",
       "240041      0.000000\n",
       "396601      0.000000\n",
       "289404      0.000000\n",
       "396631      0.000000\n",
       "396630      0.000000\n",
       "396629      0.000000\n",
       "396621      0.000000\n",
       "117162      0.000000\n",
       "289411      0.000000\n",
       "117172      0.000000\n",
       "396608      0.000000\n",
       "396607      0.000000\n",
       "240034      0.000000\n",
       "382421      0.000000\n",
       "54617            NaN\n",
       "98526            NaN\n",
       "184209           NaN\n",
       "184591           NaN\n",
       "213466           NaN\n",
       "231516           NaN\n",
       "267463           NaN\n",
       "304808           NaN\n",
       "351847           NaN\n",
       "356724           NaN\n",
       "427756           NaN\n",
       "433822           NaN\n",
       "469663           NaN\n",
       "494789           NaN\n",
       "Name: quantityModificationsPerEuro, Length: 498121, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.sort_values(by = \"quantityModificationsPerEuro\", ascending = False)['quantityModificationsPerEuro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[test['grandTotal'] == 0.00, 'secondsPerEuro'] = test['totalScanTimeInSeconds'] / 0.01\n",
    "test.loc[test['grandTotal'] == 0.00, 'lineItemVoidsPerEuro'] = test['lineItemVoids'] / 0.01\n",
    "test.loc[test['grandTotal'] == 0.00, 'scansWithoutRegistrationPerEuro'] = test['scansWithoutRegistration'] / 0.01\n",
    "test.loc[test['grandTotal'] == 0.00, 'quantityModificationsPerEuro'] = test['quantityModifications'] / 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31676     500.000000\n",
       "112020    500.000000\n",
       "287909    500.000000\n",
       "214052    500.000000\n",
       "455538    500.000000\n",
       "46561     500.000000\n",
       "53667     500.000000\n",
       "208504    500.000000\n",
       "52962     500.000000\n",
       "330870    500.000000\n",
       "270427    500.000000\n",
       "133426    500.000000\n",
       "497223    500.000000\n",
       "461791    500.000000\n",
       "143886    500.000000\n",
       "389520    400.000000\n",
       "328053    400.000000\n",
       "147774    400.000000\n",
       "3282      400.000000\n",
       "161945    400.000000\n",
       "91248     400.000000\n",
       "371320    400.000000\n",
       "118032    400.000000\n",
       "176008    400.000000\n",
       "283861    400.000000\n",
       "34895     400.000000\n",
       "43649     400.000000\n",
       "337680    400.000000\n",
       "19792     400.000000\n",
       "299044    400.000000\n",
       "13695     400.000000\n",
       "102618    400.000000\n",
       "236301    300.000000\n",
       "156517    300.000000\n",
       "462632    300.000000\n",
       "211326    300.000000\n",
       "129930    300.000000\n",
       "436235    300.000000\n",
       "428300    300.000000\n",
       "256995    300.000000\n",
       "129034    300.000000\n",
       "496219    300.000000\n",
       "58189     300.000000\n",
       "104670    300.000000\n",
       "492399    300.000000\n",
       "199015    300.000000\n",
       "16726     300.000000\n",
       "117536    300.000000\n",
       "86833     300.000000\n",
       "448922    300.000000\n",
       "441018    300.000000\n",
       "158961    300.000000\n",
       "65523     250.000000\n",
       "306285    250.000000\n",
       "449290    250.000000\n",
       "416085    250.000000\n",
       "356924    250.000000\n",
       "175423    250.000000\n",
       "57386     250.000000\n",
       "166233    250.000000\n",
       "124665    200.000000\n",
       "16998     200.000000\n",
       "64223     200.000000\n",
       "179013    200.000000\n",
       "489766    200.000000\n",
       "25121     200.000000\n",
       "16134     200.000000\n",
       "267039    200.000000\n",
       "23336     200.000000\n",
       "431434    200.000000\n",
       "35561     200.000000\n",
       "487253    200.000000\n",
       "481847    200.000000\n",
       "146252    200.000000\n",
       "149198    200.000000\n",
       "304527    200.000000\n",
       "232443    200.000000\n",
       "87759     200.000000\n",
       "373593    200.000000\n",
       "190225    200.000000\n",
       "120191    200.000000\n",
       "429833    200.000000\n",
       "306866    200.000000\n",
       "157456    200.000000\n",
       "207351    200.000000\n",
       "24165     200.000000\n",
       "374213    200.000000\n",
       "234038    200.000000\n",
       "39654     200.000000\n",
       "201272    200.000000\n",
       "456994    200.000000\n",
       "178268    200.000000\n",
       "227115    200.000000\n",
       "250987    200.000000\n",
       "7624      200.000000\n",
       "438266    200.000000\n",
       "309508    200.000000\n",
       "154649    166.666667\n",
       "286758    166.666667\n",
       "223221    166.666667\n",
       "150499    166.666667\n",
       "302576    166.666667\n",
       "101060    166.666667\n",
       "447408    166.666667\n",
       "128211    166.666667\n",
       "9273      150.000000\n",
       "469526    150.000000\n",
       "251164    150.000000\n",
       "195198    150.000000\n",
       "134886    150.000000\n",
       "22192     150.000000\n",
       "14751     150.000000\n",
       "68346     150.000000\n",
       "50403     150.000000\n",
       "103050    150.000000\n",
       "486320    133.333333\n",
       "252849    133.333333\n",
       "186307    133.333333\n",
       "315680    133.333333\n",
       "3653      133.333333\n",
       "380463    133.333333\n",
       "354611    133.333333\n",
       "82156     133.333333\n",
       "184479    133.333333\n",
       "211776    133.333333\n",
       "35380     133.333333\n",
       "136661    133.333333\n",
       "486833    133.333333\n",
       "228560    133.333333\n",
       "456458    125.000000\n",
       "288380    125.000000\n",
       "258338    125.000000\n",
       "400587    125.000000\n",
       "215024    125.000000\n",
       "81978     125.000000\n",
       "48123     125.000000\n",
       "216483    125.000000\n",
       "484382    125.000000\n",
       "195437    125.000000\n",
       "280337    125.000000\n",
       "389981    100.000000\n",
       "13725     100.000000\n",
       "455867    100.000000\n",
       "332910    100.000000\n",
       "71060     100.000000\n",
       "35932     100.000000\n",
       "418922    100.000000\n",
       "398228    100.000000\n",
       "257586    100.000000\n",
       "458442    100.000000\n",
       "106855    100.000000\n",
       "347724    100.000000\n",
       "470823    100.000000\n",
       "353291    100.000000\n",
       "158772    100.000000\n",
       "103595    100.000000\n",
       "457265    100.000000\n",
       "29990     100.000000\n",
       "454704    100.000000\n",
       "256054    100.000000\n",
       "286247    100.000000\n",
       "454691    100.000000\n",
       "372754    100.000000\n",
       "422628    100.000000\n",
       "372795    100.000000\n",
       "87373     100.000000\n",
       "30187     100.000000\n",
       "427150    100.000000\n",
       "160837    100.000000\n",
       "138846    100.000000\n",
       "242300    100.000000\n",
       "310894    100.000000\n",
       "285196    100.000000\n",
       "187633    100.000000\n",
       "405388    100.000000\n",
       "126292    100.000000\n",
       "70209     100.000000\n",
       "363211    100.000000\n",
       "215335    100.000000\n",
       "69401     100.000000\n",
       "294177    100.000000\n",
       "261011    100.000000\n",
       "278522    100.000000\n",
       "396734    100.000000\n",
       "155622    100.000000\n",
       "269421    100.000000\n",
       "40562     100.000000\n",
       "347490    100.000000\n",
       "7605      100.000000\n",
       "391369    100.000000\n",
       "30423     100.000000\n",
       "451236     83.333333\n",
       "219308     83.333333\n",
       "446393     83.333333\n",
       "485587     83.333333\n",
       "434975     83.333333\n",
       "206802     83.333333\n",
       "116391     83.333333\n",
       "306261     83.333333\n",
       "349243     83.333333\n",
       "445151     83.333333\n",
       "351935     80.000000\n",
       "233295     80.000000\n",
       "228967     80.000000\n",
       "97101      80.000000\n",
       "116881     75.000000\n",
       "467226     75.000000\n",
       "455728     75.000000\n",
       "416801     75.000000\n",
       "292051     75.000000\n",
       "314855     75.000000\n",
       "33592      75.000000\n",
       "241432     75.000000\n",
       "201233     75.000000\n",
       "378908     71.428571\n",
       "186711     71.428571\n",
       "34321      71.428571\n",
       "444529     71.428571\n",
       "379820     71.428571\n",
       "214139     71.428571\n",
       "345096     71.428571\n",
       "42831      71.428571\n",
       "150453     71.428571\n",
       "452081     71.428571\n",
       "112396     66.666667\n",
       "12561      66.666667\n",
       "428532     66.666667\n",
       "166380     66.666667\n",
       "466514     66.666667\n",
       "480748     66.666667\n",
       "60423      66.666667\n",
       "61186      66.666667\n",
       "432282     66.666667\n",
       "184687     66.666667\n",
       "205210     66.666667\n",
       "151093     66.666667\n",
       "247837     66.666667\n",
       "105383     66.666667\n",
       "50267      66.666667\n",
       "25381      66.666667\n",
       "382411     66.666667\n",
       "294199     66.666667\n",
       "85390      66.666667\n",
       "481154     66.666667\n",
       "275316     66.666667\n",
       "71334      66.666667\n",
       "269630     66.666667\n",
       "301891     62.500000\n",
       "71207      62.500000\n",
       "120256     62.500000\n",
       "             ...    \n",
       "380552      0.000000\n",
       "380547      0.000000\n",
       "380544      0.000000\n",
       "380539      0.000000\n",
       "135626      0.000000\n",
       "380533      0.000000\n",
       "380532      0.000000\n",
       "135629      0.000000\n",
       "380528      0.000000\n",
       "135643      0.000000\n",
       "136171      0.000000\n",
       "380058      0.000000\n",
       "136891      0.000000\n",
       "136697      0.000000\n",
       "379644      0.000000\n",
       "136666      0.000000\n",
       "379641      0.000000\n",
       "136669      0.000000\n",
       "136670      0.000000\n",
       "136674      0.000000\n",
       "379636      0.000000\n",
       "379632      0.000000\n",
       "379630      0.000000\n",
       "136681      0.000000\n",
       "379621      0.000000\n",
       "136696      0.000000\n",
       "379612      0.000000\n",
       "136665      0.000000\n",
       "136701      0.000000\n",
       "379610      0.000000\n",
       "136702      0.000000\n",
       "136703      0.000000\n",
       "136704      0.000000\n",
       "379606      0.000000\n",
       "136709      0.000000\n",
       "379602      0.000000\n",
       "379600      0.000000\n",
       "379590      0.000000\n",
       "136722      0.000000\n",
       "136724      0.000000\n",
       "379645      0.000000\n",
       "136664      0.000000\n",
       "379582      0.000000\n",
       "379694      0.000000\n",
       "136580      0.000000\n",
       "379722      0.000000\n",
       "136589      0.000000\n",
       "136597      0.000000\n",
       "136599      0.000000\n",
       "379710      0.000000\n",
       "136601      0.000000\n",
       "379706      0.000000\n",
       "379705      0.000000\n",
       "379703      0.000000\n",
       "136608      0.000000\n",
       "379695      0.000000\n",
       "136609      0.000000\n",
       "136663      0.000000\n",
       "379684      0.000000\n",
       "136614      0.000000\n",
       "136619      0.000000\n",
       "136620      0.000000\n",
       "379680      0.000000\n",
       "136625      0.000000\n",
       "136627      0.000000\n",
       "379669      0.000000\n",
       "379668      0.000000\n",
       "136650      0.000000\n",
       "379655      0.000000\n",
       "379653      0.000000\n",
       "379583      0.000000\n",
       "136728      0.000000\n",
       "380057      0.000000\n",
       "136854      0.000000\n",
       "379487      0.000000\n",
       "379485      0.000000\n",
       "136829      0.000000\n",
       "379482      0.000000\n",
       "379480      0.000000\n",
       "379475      0.000000\n",
       "136840      0.000000\n",
       "379469      0.000000\n",
       "379468      0.000000\n",
       "136851      0.000000\n",
       "136852      0.000000\n",
       "379462      0.000000\n",
       "136855      0.000000\n",
       "136826      0.000000\n",
       "379456      0.000000\n",
       "136857      0.000000\n",
       "136858      0.000000\n",
       "379451      0.000000\n",
       "136861      0.000000\n",
       "136864      0.000000\n",
       "136867      0.000000\n",
       "379444      0.000000\n",
       "136874      0.000000\n",
       "379439      0.000000\n",
       "136878      0.000000\n",
       "136879      0.000000\n",
       "136828      0.000000\n",
       "379498      0.000000\n",
       "379579      0.000000\n",
       "136768      0.000000\n",
       "136729      0.000000\n",
       "136737      0.000000\n",
       "379573      0.000000\n",
       "136745      0.000000\n",
       "136750      0.000000\n",
       "136752      0.000000\n",
       "136761      0.000000\n",
       "136763      0.000000\n",
       "379557      0.000000\n",
       "379554      0.000000\n",
       "379553      0.000000\n",
       "379551      0.000000\n",
       "136773      0.000000\n",
       "136814      0.000000\n",
       "136776      0.000000\n",
       "379540      0.000000\n",
       "379532      0.000000\n",
       "379527      0.000000\n",
       "136784      0.000000\n",
       "379519      0.000000\n",
       "136788      0.000000\n",
       "136789      0.000000\n",
       "379514      0.000000\n",
       "136790      0.000000\n",
       "379505      0.000000\n",
       "136809      0.000000\n",
       "379730      0.000000\n",
       "379732      0.000000\n",
       "379734      0.000000\n",
       "136350      0.000000\n",
       "136324      0.000000\n",
       "136326      0.000000\n",
       "136335      0.000000\n",
       "136337      0.000000\n",
       "379948      0.000000\n",
       "136338      0.000000\n",
       "379946      0.000000\n",
       "136345      0.000000\n",
       "379936      0.000000\n",
       "379932      0.000000\n",
       "379931      0.000000\n",
       "136348      0.000000\n",
       "379926      0.000000\n",
       "379957      0.000000\n",
       "136353      0.000000\n",
       "136354      0.000000\n",
       "136356      0.000000\n",
       "379917      0.000000\n",
       "136362      0.000000\n",
       "379914      0.000000\n",
       "136367      0.000000\n",
       "136369      0.000000\n",
       "379908      0.000000\n",
       "136370      0.000000\n",
       "379906      0.000000\n",
       "379897      0.000000\n",
       "379956      0.000000\n",
       "136316      0.000000\n",
       "136575      0.000000\n",
       "136239      0.000000\n",
       "136180      0.000000\n",
       "380053      0.000000\n",
       "380048      0.000000\n",
       "380041      0.000000\n",
       "136197      0.000000\n",
       "136216      0.000000\n",
       "380031      0.000000\n",
       "380028      0.000000\n",
       "136227      0.000000\n",
       "136232      0.000000\n",
       "136236      0.000000\n",
       "380017      0.000000\n",
       "136240      0.000000\n",
       "379974      0.000000\n",
       "136260      0.000000\n",
       "380003      0.000000\n",
       "136267      0.000000\n",
       "379997      0.000000\n",
       "136279      0.000000\n",
       "379993      0.000000\n",
       "136280      0.000000\n",
       "136281      0.000000\n",
       "136282      0.000000\n",
       "136286      0.000000\n",
       "379977      0.000000\n",
       "379976      0.000000\n",
       "379896      0.000000\n",
       "379895      0.000000\n",
       "136391      0.000000\n",
       "379782      0.000000\n",
       "136482      0.000000\n",
       "379812      0.000000\n",
       "379809      0.000000\n",
       "379808      0.000000\n",
       "136491      0.000000\n",
       "379804      0.000000\n",
       "136501      0.000000\n",
       "136507      0.000000\n",
       "379799      0.000000\n",
       "136511      0.000000\n",
       "136514      0.000000\n",
       "379785      0.000000\n",
       "379779      0.000000\n",
       "136400      0.000000\n",
       "136522      0.000000\n",
       "379777      0.000000\n",
       "379774      0.000000\n",
       "136524      0.000000\n",
       "379772      0.000000\n",
       "136525      0.000000\n",
       "136527      0.000000\n",
       "379758      0.000000\n",
       "136547      0.000000\n",
       "136551      0.000000\n",
       "379749      0.000000\n",
       "136570      0.000000\n",
       "136476      0.000000\n",
       "379821      0.000000\n",
       "379825      0.000000\n",
       "379826      0.000000\n",
       "136403      0.000000\n",
       "379874      0.000000\n",
       "136408      0.000000\n",
       "136417      0.000000\n",
       "136422      0.000000\n",
       "379865      0.000000\n",
       "136425      0.000000\n",
       "136429      0.000000\n",
       "136430      0.000000\n",
       "136434      0.000000\n",
       "136443      0.000000\n",
       "379857      0.000000\n",
       "379853      0.000000\n",
       "136450      0.000000\n",
       "379847      0.000000\n",
       "379844      0.000000\n",
       "379840      0.000000\n",
       "136464      0.000000\n",
       "379837      0.000000\n",
       "136466      0.000000\n",
       "136468      0.000000\n",
       "379834      0.000000\n",
       "379831      0.000000\n",
       "379828      0.000000\n",
       "379827      0.000000\n",
       "133509      0.000000\n",
       "Name: quantityModificationsPerEuro, Length: 498121, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.sort_values(by = \"quantityModificationsPerEuro\", ascending = False)['quantityModificationsPerEuro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unbuffered(object):\n",
    "    def __init__(self, stream):\n",
    "        self.stream = stream\n",
    "    def write(self, data):\n",
    "        self.stream.write(data)\n",
    "        self.stream.flush()\n",
    "    def __getattr__(self, attr):\n",
    "        return getattr(self.stream, attr)\n",
    "\n",
    "import sys\n",
    "sys.stdout = Unbuffered(sys.stdout)\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "import numpy\n",
    "import sklearn.metrics\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "import nlopt\n",
    "import scipy.stats\n",
    "\n",
    "class CPLELearningModel(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Contrastive Pessimistic Likelihood Estimation framework for semi-supervised \n",
    "    learning, based on (Loog, 2015). This implementation contains two \n",
    "    significant differences to (Loog, 2015):\n",
    "    - the discriminative likelihood p(y|X), instead of the generative \n",
    "    likelihood p(X), is used for optimization\n",
    "    - apart from `pessimism' (the assumption that the true labels of the \n",
    "    unlabeled instances are as adversarial to the likelihood as possible), the \n",
    "    optimization objective also tries to increase the likelihood on the labeled\n",
    "    examples\n",
    "\n",
    "    This class takes a base model (any scikit learn estimator),\n",
    "    trains it on the labeled examples, and then uses global optimization to \n",
    "    find (soft) label hypotheses for the unlabeled examples in a pessimistic  \n",
    "    fashion (such that the model log likelihood on the unlabeled data is as  \n",
    "    small as possible, but the log likelihood on the labeled data is as high \n",
    "    as possible)\n",
    "\n",
    "    See Loog, Marco. \"Contrastive Pessimistic Likelihood Estimation for \n",
    "    Semi-Supervised Classification.\" arXiv preprint arXiv:1503.00269 (2015).\n",
    "    http://arxiv.org/pdf/1503.00269\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    basemodel : BaseEstimator instance\n",
    "        Base classifier to be trained on the partially supervised data\n",
    "\n",
    "    pessimistic : boolean, optional (default=True)\n",
    "        Whether the label hypotheses for the unlabeled instances should be\n",
    "        pessimistic (i.e. minimize log likelihood) or optimistic (i.e. \n",
    "        maximize log likelihood).\n",
    "        Pessimistic label hypotheses ensure safety (i.e. the semi-supervised\n",
    "        solution will not be worse than a model trained on the purely \n",
    "        supervised instances)\n",
    "        \n",
    "    predict_from_probabilities : boolean, optional (default=False)\n",
    "        The prediction is calculated from the probabilities if this is True \n",
    "        (1 if more likely than the mean predicted probability or 0 otherwise).\n",
    "        If it is false, the normal base model predictions are used.\n",
    "        This only affects the predict function. Warning: only set to true if \n",
    "        predict will be called with a substantial number of data points\n",
    "        \n",
    "    use_sample_weighting : boolean, optional (default=True)\n",
    "        Whether to use sample weights (soft labels) for the unlabeled instances.\n",
    "        Setting this to False allows the use of base classifiers which do not\n",
    "        support sample weights (but might slow down the optimization)\n",
    "\n",
    "    max_iter : int, optional (default=3000)\n",
    "        Maximum number of iterations\n",
    "        \n",
    "    verbose : int, optional (default=1)\n",
    "        Enable verbose output (1 shows progress, 2 shows the detailed log \n",
    "        likelihood at every iteration).\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, basemodel, pessimistic=True, predict_from_probabilities = False, use_sample_weighting = True, max_iter=3000, verbose = 1):\n",
    "        self.model = basemodel\n",
    "        self.pessimistic = pessimistic\n",
    "        self.predict_from_probabilities = predict_from_probabilities\n",
    "        self.use_sample_weighting = use_sample_weighting\n",
    "        self.max_iter = max_iter\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.it = 0 # iteration counter\n",
    "        self.noimprovementsince = 0 # log likelihood hasn't improved since this number of iterations\n",
    "        self.maxnoimprovementsince = 3 # threshold for iterations without improvements (convergence is assumed when this is reached)\n",
    "        \n",
    "        self.buffersize = 200\n",
    "        # buffer for the last few discriminative likelihoods (used to check for convergence)\n",
    "        self.lastdls = [0]*self.buffersize\n",
    "        \n",
    "        # best discriminative likelihood and corresponding soft labels; updated during training\n",
    "        self.bestdl = numpy.infty\n",
    "        self.bestlbls = []\n",
    "        \n",
    "        # unique id\n",
    "        self.id = str(chr(numpy.random.randint(26)+97))+str(chr(numpy.random.randint(26)+97))\n",
    "\n",
    "    def discriminative_likelihood(self, model, labeledData, labeledy = None, unlabeledData = None, unlabeledWeights = None, unlabeledlambda = 1, gradient=[], alpha = 0.01):\n",
    "        unlabeledy = (unlabeledWeights[:, 0]<0.5)*1\n",
    "        uweights = numpy.copy(unlabeledWeights[:, 0]) # large prob. for k=0 instances, small prob. for k=1 instances \n",
    "        uweights[unlabeledy==1] = 1-uweights[unlabeledy==1] # subtract from 1 for k=1 instances to reflect confidence\n",
    "        weights = numpy.hstack((numpy.ones(len(labeledy)), uweights))\n",
    "        labels = numpy.hstack((labeledy, unlabeledy))\n",
    "        \n",
    "        # fit model on supervised data\n",
    "        if self.use_sample_weighting:\n",
    "            model.fit(numpy.vstack((labeledData, unlabeledData)), labels, sample_weight=weights)\n",
    "        else:\n",
    "            model.fit(numpy.vstack((labeledData, unlabeledData)), labels)\n",
    "        \n",
    "        # probability of labeled data\n",
    "        P = model.predict_proba(labeledData)\n",
    "        \n",
    "        try:\n",
    "            # labeled discriminative log likelihood\n",
    "            labeledDL = -sklearn.metrics.log_loss(labeledy, P)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            P = model.predict_proba(labeledData)\n",
    "\n",
    "        # probability of unlabeled data\n",
    "        unlabeledP = model.predict_proba(unlabeledData)  \n",
    "           \n",
    "        try:\n",
    "            # unlabeled discriminative log likelihood\n",
    "            eps = 1e-15\n",
    "            unlabeledP = numpy.clip(unlabeledP, eps, 1 - eps)\n",
    "            unlabeledDL = numpy.average((unlabeledWeights*numpy.vstack((1-unlabeledy, unlabeledy)).T*numpy.log(unlabeledP)).sum(axis=1))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            unlabeledP = model.predict_proba(unlabeledData)\n",
    "        \n",
    "        if self.pessimistic:\n",
    "            # pessimistic: minimize the difference between unlabeled and labeled discriminative likelihood (assume worst case for unknown true labels)\n",
    "            dl = unlabeledlambda * unlabeledDL - labeledDL\n",
    "        else: \n",
    "            # optimistic: minimize negative total discriminative likelihood (i.e. maximize likelihood) \n",
    "            dl = - unlabeledlambda * unlabeledDL - labeledDL\n",
    "        \n",
    "        return dl\n",
    "        \n",
    "    def discriminative_likelihood_objective(self, model, labeledData, labeledy = None, unlabeledData = None, unlabeledWeights = None, unlabeledlambda = 1, gradient=[], alpha = 0.01):\n",
    "        if self.it == 0:\n",
    "            self.lastdls = [0]*self.buffersize\n",
    "        \n",
    "        dl = self.discriminative_likelihood(model, labeledData, labeledy, unlabeledData, unlabeledWeights, unlabeledlambda, gradient, alpha)\n",
    "        \n",
    "        self.it += 1\n",
    "        self.lastdls[numpy.mod(self.it, len(self.lastdls))] = dl\n",
    "        \n",
    "        if numpy.mod(self.it, self.buffersize) == 0: # or True:\n",
    "            improvement = numpy.mean((self.lastdls[(len(self.lastdls)/2):])) - numpy.mean((self.lastdls[:(len(self.lastdls)/2)]))\n",
    "            # ttest - test for hypothesis that the likelihoods have not changed (i.e. there has been no improvement, and we are close to convergence) \n",
    "            _, prob = scipy.stats.ttest_ind(self.lastdls[(len(self.lastdls)/2):], self.lastdls[:(len(self.lastdls)/2)])\n",
    "            \n",
    "            # if improvement is not certain accoring to t-test...\n",
    "            noimprovement = prob > 0.1 and numpy.mean(self.lastdls[(len(self.lastdls)/2):]) < numpy.mean(self.lastdls[:(len(self.lastdls)/2)])\n",
    "            if noimprovement:\n",
    "                self.noimprovementsince += 1\n",
    "                if self.noimprovementsince >= self.maxnoimprovementsince:\n",
    "                    # no improvement since a while - converged; exit\n",
    "                    self.noimprovementsince = 0\n",
    "                    raise Exception(\" converged.\") # we need to raise an exception to get NLopt to stop before exceeding the iteration budget\n",
    "            else:\n",
    "                self.noimprovementsince = 0\n",
    "            \n",
    "            if self.verbose == 2:\n",
    "                print(self.id,self.it, dl, numpy.mean(self.lastdls), improvement, round(prob, 3), (prob < 0.1))\n",
    "            elif self.verbose:\n",
    "                sys.stdout.write(('.' if self.pessimistic else '.') if not noimprovement else 'n')\n",
    "                      \n",
    "        if dl < self.bestdl:\n",
    "            self.bestdl = dl\n",
    "            self.bestlbls = numpy.copy(unlabeledWeights[:, 0])\n",
    "                        \n",
    "        return dl\n",
    "    \n",
    "    def fit(self, X, y): # -1 for unlabeled\n",
    "        unlabeledX = X[y==-1, :]\n",
    "        labeledX = X[y!=-1, :]\n",
    "        labeledy = y[y!=-1]\n",
    "        \n",
    "        M = unlabeledX.shape[0]\n",
    "        \n",
    "        # train on labeled data\n",
    "        self.model.fit(labeledX, labeledy)\n",
    "\n",
    "        unlabeledy = self.predict(unlabeledX)\n",
    "        \n",
    "        #re-train, labeling unlabeled instances pessimistically\n",
    "        \n",
    "        # pessimistic soft labels ('weights') q for unlabelled points, q=P(k=0|Xu)\n",
    "        f = lambda softlabels, grad=[]: self.discriminative_likelihood_objective(self.model, labeledX, labeledy=labeledy, unlabeledData=unlabeledX, unlabeledWeights=numpy.vstack((softlabels, 1-softlabels)).T, gradient=grad) #- supLL\n",
    "        lblinit = numpy.random.random(len(unlabeledy))\n",
    "\n",
    "        try:\n",
    "            self.it = 0\n",
    "            opt = nlopt.opt(nlopt.GN_DIRECT_L_RAND, M)\n",
    "            opt.set_lower_bounds(numpy.zeros(M))\n",
    "            opt.set_upper_bounds(numpy.ones(M))\n",
    "            opt.set_min_objective(f)\n",
    "            opt.set_maxeval(self.max_iter)\n",
    "            self.bestsoftlbl = opt.optimize(lblinit)\n",
    "            print(\" max_iter exceeded.\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            self.bestsoftlbl = self.bestlbls\n",
    "            \n",
    "        if numpy.any(self.bestsoftlbl != self.bestlbls):\n",
    "            self.bestsoftlbl = self.bestlbls\n",
    "        ll = f(self.bestsoftlbl)\n",
    "\n",
    "        unlabeledy = (self.bestsoftlbl<0.5)*1\n",
    "        uweights = numpy.copy(self.bestsoftlbl) # large prob. for k=0 instances, small prob. for k=1 instances \n",
    "        uweights[unlabeledy==1] = 1-uweights[unlabeledy==1] # subtract from 1 for k=1 instances to reflect confidence\n",
    "        weights = numpy.hstack((numpy.ones(len(labeledy)), uweights))\n",
    "        labels = numpy.hstack((labeledy, unlabeledy))\n",
    "        if self.use_sample_weighting:\n",
    "            self.model.fit(numpy.vstack((labeledX, unlabeledX)), labels, sample_weight=weights)\n",
    "        else:\n",
    "            self.model.fit(numpy.vstack((labeledX, unlabeledX)), labels)\n",
    "        \n",
    "        if self.verbose > 1:\n",
    "            print(\"number of non-one soft labels: \", numpy.sum(self.bestsoftlbl != 1), \", balance:\", numpy.sum(self.bestsoftlbl<0.5), \" / \", len(self.bestsoftlbl))\n",
    "            print(\"current likelihood: \", ll)\n",
    "        \n",
    "        if not getattr(self.model, \"predict_proba\", None):\n",
    "            # Platt scaling\n",
    "            self.plattlr = LR()\n",
    "            preds = self.model.predict(labeledX)\n",
    "            self.plattlr.fit( preds.reshape( -1, 1 ), labeledy )\n",
    "            \n",
    "        return self\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Compute probabilities of possible outcomes for samples in X.\n",
    "\n",
    "        The model need to have probability information computed at training\n",
    "        time: fit with attribute `probability` set to True.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        T : array-like, shape = [n_samples, n_classes]\n",
    "            Returns the probability of the sample for each class in\n",
    "            the model. The columns correspond to the classes in sorted\n",
    "            order, as they appear in the attribute `classes_`.\n",
    "        \"\"\"\n",
    "        \n",
    "        if getattr(self.model, \"predict_proba\", None):\n",
    "            return self.model.predict_proba(X)\n",
    "        else:\n",
    "            preds = self.model.predict(X)\n",
    "            return self.plattlr.predict_proba(preds.reshape( -1, 1 ))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Perform classification on samples in X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : array, shape = [n_samples]\n",
    "            Class labels for samples in X.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.predict_from_probabilities:\n",
    "            P = self.predict_proba(X)\n",
    "            return (P[:, 0]<numpy.average(P[:, 0]))\n",
    "        else:\n",
    "            return self.model.predict(X)\n",
    "    \n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        return sklearn.metrics.accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monetary_value(cm):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        print(\"True negative: \", tn)\n",
    "        print(\"False positive: \", fp)\n",
    "        print(\"False negative: \", fn)\n",
    "        print(\"True positive: \", tp)\n",
    "        score = (-25)*fp + (-5)*fn + 5*tp\n",
    "        print(str(score) + \" for \" + str(sum(sum(cm))) + \" instances in the test set\")\n",
    "        print(str(score/sum(sum(cm))) + \" per instance in the test set\")\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "True negative:  440\n",
      "False positive:  2\n",
      "False negative:  5\n",
      "True positive:  23\n",
      "40 for 470 instances in the test set\n",
      "0.0851063829787234 per instance in the test set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X = train.drop('fraud', axis=1)\n",
    "y = train.fraud\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "basemodel = LogisticRegression(max_iter = 10000, solver = 'newton-cg')\n",
    "basemodel.fit(X_train, y_train)\n",
    "cm = (confusion_matrix(y_test, basemodel.predict(X_test)))\n",
    "\n",
    "print(\"Logistic Regression:\")\n",
    "get_monetary_value(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slice indices must be integers or None or have an __index__ method\n",
      "CPLE:\n",
      "True negative:  442\n",
      "False positive:  0\n",
      "False negative:  28\n",
      "True positive:  0\n",
      "-140 for 470 instances in the test set\n",
      "-0.2978723404255319 per instance in the test set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-140"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['fraud']=-1\n",
    "X_train_combined = pd.concat([X_train, test.drop('fraud', axis=1)])\n",
    "y_train_combined = pd.concat([y_train, test.fraud])\n",
    "\n",
    "#convert to numpy-array\n",
    "#for test-purposes limit samples because it takes ages\n",
    "npX_train_combined = X_train_combined[:5000].values\n",
    "npy_train_combined = y_train_combined[:5000].values\n",
    "\n",
    "\n",
    "ssmodel = CPLELearningModel(basemodel)\n",
    "ssmodel.fit(npX_train_combined, npy_train_combined)\n",
    "cm = (confusion_matrix(y_test, ssmodel.predict(X_test)))\n",
    "\n",
    "print(\"CPLE:\")\n",
    "get_monetary_value(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
